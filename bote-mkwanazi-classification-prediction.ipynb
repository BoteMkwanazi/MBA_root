{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-05T17:33:52.70209Z","iopub.execute_input":"2021-10-05T17:33:52.702547Z","iopub.status.idle":"2021-10-05T17:33:52.712599Z","shell.execute_reply.started":"2021-10-05T17:33:52.702502Z","shell.execute_reply":"2021-10-05T17:33:52.711975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TABLE OF CONTENTS \n\n<a id='table'></a>\n### 1. [Importing Libraries](#libraries)  \n\n### 2. [Loading Data](#train_and_test)  \n    \n### 3. [Cleaning Data](#cleaning)  \n     \n### 4. [Exploratory Data Analysis](#EDA)\n\n### 5. [Feature Engineering](#extraction)\n\n### 6. [Model Training](#training)\n\n### 7. [Model Results](#findings)\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries\n<a id='libraries'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"# Libraries used to load dataframe and visualize data\nimport numpy as np \nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.features import RadViz\nfrom wordcloud import WordCloud\nimport plotly.io as pio\npio.renderers.default='notebook'\n%matplotlib inline\n\n# Noise removal helper libraries\nimport re\nimport string \nfrom stopwordsiso import stopwords as sw\nfrom nltk.corpus import stopwords\n\n# Text Preprocessing\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Feature Engineering and Data preparation for modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n# Model building and training\nfrom sklearn.svm import SVC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Model evaluation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n#save the final model and vectorizer\nimport pickle\n\n# width_size\ncontext = pd.option_context('display.max_colwidth', 400)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:25.024035Z","iopub.execute_input":"2021-10-05T17:35:25.024391Z","iopub.status.idle":"2021-10-05T17:35:25.179984Z","shell.execute_reply.started":"2021-10-05T17:35:25.02435Z","shell.execute_reply":"2021-10-05T17:35:25.179106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install stopwordsiso\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:34:55.478193Z","iopub.execute_input":"2021-10-05T17:34:55.478508Z","iopub.status.idle":"2021-10-05T17:35:05.623043Z","shell.execute_reply.started":"2021-10-05T17:34:55.478476Z","shell.execute_reply":"2021-10-05T17:35:05.622164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Loading Data\n<a id='train_and_test'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"# Loading train and test dataframes\ntrain_df = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:49.728355Z","iopub.execute_input":"2021-10-05T17:35:49.728965Z","iopub.status.idle":"2021-10-05T17:35:49.889838Z","shell.execute_reply.started":"2021-10-05T17:35:49.728924Z","shell.execute_reply":"2021-10-05T17:35:49.888931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first 10 rows training dataset dataframe, allowing maximum width for the message column\nwith context:\n    display(train_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:51.018964Z","iopub.execute_input":"2021-10-05T17:35:51.019241Z","iopub.status.idle":"2021-10-05T17:35:51.04325Z","shell.execute_reply.started":"2021-10-05T17:35:51.019212Z","shell.execute_reply":"2021-10-05T17:35:51.042414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first 10 rows testing dataset dataframe, allowing maximum width for the message column\nwith context:\n    display(test_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:53.508444Z","iopub.execute_input":"2021-10-05T17:35:53.50876Z","iopub.status.idle":"2021-10-05T17:35:53.518954Z","shell.execute_reply.started":"2021-10-05T17:35:53.508728Z","shell.execute_reply":"2021-10-05T17:35:53.518052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Cleaning Data\n<a id='cleaning'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"# Create function to clean data\ndef clean_data(df):\n    \n    # removing noise with regex.\n    address = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)' \n    df.message.replace(to_replace = address, value = '', regex = True, inplace=True)\n    df.message.replace({r'@(\\w+)'}, value = '', regex = True, inplace=True)\n    df.message.replace({r'\\d+'}, value = '', regex = True, inplace=True)\n    df.message.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n    \n    # lower cases to avoid capital letters noise \n    lower_cases = lambda tweets: ''.join([i.lower() for i in tweets])\n    df['message'] = df.message.apply(lower_cases)\n    \n    # this function removes punctuation\n    punctuations = lambda tweets: ''.join([i for i in tweets if i not in string.punctuation])\n    df['message'] = df.message.apply(punctuations)\n    \n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:55.586703Z","iopub.execute_input":"2021-10-05T17:35:55.586979Z","iopub.status.idle":"2021-10-05T17:35:55.599234Z","shell.execute_reply.started":"2021-10-05T17:35:55.586951Z","shell.execute_reply":"2021-10-05T17:35:55.598291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of clean data of train dataset, allowing max of width\ntrain_df_clean = clean_data(train_df)\n\nwith context:\n    display(train_df_clean.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:56.473989Z","iopub.execute_input":"2021-10-05T17:35:56.474285Z","iopub.status.idle":"2021-10-05T17:35:57.230551Z","shell.execute_reply.started":"2021-10-05T17:35:56.474255Z","shell.execute_reply":"2021-10-05T17:35:57.229678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of clean data of test dataset\ntest_df_clean = clean_data(test_df)\n\nwith context:\n    display(test_df_clean.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:58.155364Z","iopub.execute_input":"2021-10-05T17:35:58.155666Z","iopub.status.idle":"2021-10-05T17:35:58.658351Z","shell.execute_reply.started":"2021-10-05T17:35:58.155636Z","shell.execute_reply":"2021-10-05T17:35:58.657432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create function that tokenizes the words in a dataframe\ndef tokenize(df, column):\n    df = df.copy()\n    df[column] = df[column].apply(TweetTokenizer(reduce_len = True).tokenize)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:35:59.590643Z","iopub.execute_input":"2021-10-05T17:35:59.591498Z","iopub.status.idle":"2021-10-05T17:35:59.595462Z","shell.execute_reply.started":"2021-10-05T17:35:59.591459Z","shell.execute_reply":"2021-10-05T17:35:59.594886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a tokenized training dataframe\ntrain_df_tokens = tokenize(train_df_clean, 'message')\n\nwith context:\n    display(train_df_tokens.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:36:01.631581Z","iopub.execute_input":"2021-10-05T17:36:01.632103Z","iopub.status.idle":"2021-10-05T17:36:03.645452Z","shell.execute_reply.started":"2021-10-05T17:36:01.632072Z","shell.execute_reply":"2021-10-05T17:36:03.644525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a tokenized testing dataframe\ntest_df_tokens = tokenize(test_df_clean, 'message')\n\nwith context:\n    display(test_df_tokens.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:36:04.387058Z","iopub.execute_input":"2021-10-05T17:36:04.387349Z","iopub.status.idle":"2021-10-05T17:36:05.752288Z","shell.execute_reply.started":"2021-10-05T17:36:04.38732Z","shell.execute_reply":"2021-10-05T17:36:05.751328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function that removes stopwords\ndef stop_words(df, column_name):\n    df = df.copy()\n    # Returns tokenized words that are not rt\n    returns = lambda tweets: [i for i in tweets if i != 'rt']\n    df[column_name] = df[column_name].apply(returns)\n    \n    #Create a function stops which returns the words in a tokenized dataframe that do not appear in a stopwords set\n    stop_word = lambda tweets: [i for i in tweets if i not in sw('en')]\n    df[column_name] = df[column_name].apply(stop_word)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:36:05.754426Z","iopub.execute_input":"2021-10-05T17:36:05.754745Z","iopub.status.idle":"2021-10-05T17:36:05.76176Z","shell.execute_reply.started":"2021-10-05T17:36:05.754705Z","shell.execute_reply":"2021-10-05T17:36:05.760641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the stops function the tokenized testing dataset dataframe\ntrain_df_stopwords = stop_words(train_df_tokens, 'message')\n\nwith context:\n    display(train_df_stopwords.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:36:06.51554Z","iopub.execute_input":"2021-10-05T17:36:06.515811Z","iopub.status.idle":"2021-10-05T17:36:19.668266Z","shell.execute_reply.started":"2021-10-05T17:36:06.515783Z","shell.execute_reply":"2021-10-05T17:36:19.667402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_stopwords = stop_words(test_df_tokens, 'message')\n\nwith context:\n    display(test_df_stopwords.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:36:19.669874Z","iopub.execute_input":"2021-10-05T17:36:19.670737Z","iopub.status.idle":"2021-10-05T17:36:28.194033Z","shell.execute_reply.started":"2021-10-05T17:36:19.6707Z","shell.execute_reply":"2021-10-05T17:36:28.193152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to lemmatize words in training dataframe\ntrain_df_lemmatized = train_df_stopwords.copy()\n\ntrain_df_lemmatized['message'] = train_df_lemmatized['message'].apply(lambda sentence : [WordNetLemmatizer().lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maxmimum width for the message column\nwith context:\n    display(train_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:40:20.789545Z","iopub.execute_input":"2021-10-05T17:40:20.789843Z","iopub.status.idle":"2021-10-05T17:40:21.544024Z","shell.execute_reply.started":"2021-10-05T17:40:20.789813Z","shell.execute_reply":"2021-10-05T17:40:21.543447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to lemmatize words in training dataframe\ntest_df_lemmatized = test_df_stopwords.copy()\n\ntest_df_lemmatized['message'] = test_df_lemmatized['message'].apply(lambda sentence : [WordNetLemmatizer().lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maxmimum width for the message column\nwith context:\n    display(test_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T17:47:27.231689Z","iopub.execute_input":"2021-10-05T17:47:27.232571Z","iopub.status.idle":"2021-10-05T17:47:27.747735Z","shell.execute_reply.started":"2021-10-05T17:47:27.232524Z","shell.execute_reply":"2021-10-05T17:47:27.746868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}