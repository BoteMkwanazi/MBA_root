{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-10T12:05:56.120564Z","iopub.execute_input":"2021-10-10T12:05:56.120882Z","iopub.status.idle":"2021-10-10T12:05:56.133435Z","shell.execute_reply.started":"2021-10-10T12:05:56.120851Z","shell.execute_reply":"2021-10-10T12:05:56.132331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TABLE OF CONTENTS \n\n<a id='table'></a>\n### 1. [Importing Libraries](#libraries)  \n\n### 2. [Loading Data](#train_and_test)  \n    \n### 3. [Cleaning Data](#cleaning)  \n     \n### 4. [Exploratory Data Analysis](#EDA)\n\n### 5. [Feature Engineering](#extraction)\n\n### 6. [Modelling](#modelling)\n\n### 7. [Model Results](#findings)\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries\n<a id='libraries'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"pip install stopwordsiso","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:05:56.135464Z","iopub.execute_input":"2021-10-10T12:05:56.136355Z","iopub.status.idle":"2021-10-10T12:06:05.432872Z","shell.execute_reply.started":"2021-10-10T12:05:56.136287Z","shell.execute_reply":"2021-10-10T12:06:05.431787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Libraries used to load dataframe and visualize data\nimport numpy as np \nimport pandas as pd\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.features import RadViz\nfrom wordcloud import WordCloud\nimport plotly.io as pio\npio.renderers.default='notebook'\n%matplotlib inline\n\n# Noise removal helper libraries\nimport re\nimport string \nfrom stopwordsiso import stopwords as sw\nfrom nltk.corpus import stopwords\n\n# Text Preprocessing\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Feature Engineering and Data preparation for modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n# Model building and training\nfrom sklearn.svm import SVC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Model evaluation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\n\n#save the final model and vectorizer\nimport pickle\n\n# width_size\ncontext = pd.option_context('display.max_colwidth', 400)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:05.434839Z","iopub.execute_input":"2021-10-10T12:06:05.435074Z","iopub.status.idle":"2021-10-10T12:06:31.110604Z","shell.execute_reply.started":"2021-10-10T12:06:05.435045Z","shell.execute_reply":"2021-10-10T12:06:31.109697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Loading Data\n<a id='train_and_test'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"# Loading train and test dataframes\ntrain_df = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/test.csv')\ntrain = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/edsa-climate-change-belief-analysis-2021/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:31.111814Z","iopub.execute_input":"2021-10-10T12:06:31.112034Z","iopub.status.idle":"2021-10-10T12:06:31.417302Z","shell.execute_reply.started":"2021-10-10T12:06:31.112009Z","shell.execute_reply":"2021-10-10T12:06:31.416225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first 10 rows training dataset dataframe, allowing maximum width for the message column\nwith context:\n    display(train_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:31.42024Z","iopub.execute_input":"2021-10-10T12:06:31.420587Z","iopub.status.idle":"2021-10-10T12:06:31.443886Z","shell.execute_reply.started":"2021-10-10T12:06:31.420542Z","shell.execute_reply":"2021-10-10T12:06:31.443076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first 10 rows testing dataset dataframe, allowing maximum width for the message column\nwith context:\n    display(test_df.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:31.44518Z","iopub.execute_input":"2021-10-10T12:06:31.445406Z","iopub.status.idle":"2021-10-10T12:06:31.457651Z","shell.execute_reply.started":"2021-10-10T12:06:31.44538Z","shell.execute_reply":"2021-10-10T12:06:31.456636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Cleaning Data\n<a id='cleaning'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"# Create function to clean data\ndef clean_data(df):\n    \n    # removing noise with regex.\n    address = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)' \n    df.message.replace(to_replace = address, value = '', regex = True, inplace=True)\n    df.message.replace({r'@(\\w+)'}, value = '', regex = True, inplace=True)\n    df.message.replace({r'\\d+'}, value = '', regex = True, inplace=True)\n    df.message.replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n    \n    # lower cases to avoid capital letters noise \n    lower_cases = lambda tweets: ''.join([i.lower() for i in tweets])\n    df['message'] = df.message.apply(lower_cases)\n    \n    # this function removes punctuation\n    punctuations = lambda tweets: ''.join([i for i in tweets if i not in string.punctuation])\n    df['message'] = df.message.apply(punctuations)\n    \n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:31.459132Z","iopub.execute_input":"2021-10-10T12:06:31.460014Z","iopub.status.idle":"2021-10-10T12:06:31.470271Z","shell.execute_reply.started":"2021-10-10T12:06:31.459979Z","shell.execute_reply":"2021-10-10T12:06:31.469482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of clean data of train dataset, allowing max of width\ntrain_df_clean = clean_data(train_df)\n\nwith context:\n    display(train_df_clean.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:31.471338Z","iopub.execute_input":"2021-10-10T12:06:31.47224Z","iopub.status.idle":"2021-10-10T12:06:32.025351Z","shell.execute_reply.started":"2021-10-10T12:06:31.4722Z","shell.execute_reply":"2021-10-10T12:06:32.024393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of clean data of test dataset\ntest_df_clean = clean_data(test_df)\n\nwith context:\n    display(test_df_clean.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:32.026567Z","iopub.execute_input":"2021-10-10T12:06:32.026779Z","iopub.status.idle":"2021-10-10T12:06:32.39247Z","shell.execute_reply.started":"2021-10-10T12:06:32.026755Z","shell.execute_reply":"2021-10-10T12:06:32.391557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create function that tokenizes the words in a dataframe\ndef tokenize(df, column):\n    df = df.copy()\n    df[column] = df[column].apply(TweetTokenizer(reduce_len = True).tokenize)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:32.395307Z","iopub.execute_input":"2021-10-10T12:06:32.395526Z","iopub.status.idle":"2021-10-10T12:06:32.400535Z","shell.execute_reply.started":"2021-10-10T12:06:32.395501Z","shell.execute_reply":"2021-10-10T12:06:32.39948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a tokenized training dataframe\ntrain_df_tokens = tokenize(train_df_clean, 'message')\n\nwith context:\n    display(train_df_tokens.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:32.404234Z","iopub.execute_input":"2021-10-10T12:06:32.404832Z","iopub.status.idle":"2021-10-10T12:06:33.235306Z","shell.execute_reply.started":"2021-10-10T12:06:32.404792Z","shell.execute_reply":"2021-10-10T12:06:33.234144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a tokenized testing dataframe\ntest_df_tokens = tokenize(test_df_clean, 'message')\n\nwith context:\n    display(test_df_tokens.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:33.236541Z","iopub.execute_input":"2021-10-10T12:06:33.236789Z","iopub.status.idle":"2021-10-10T12:06:33.79728Z","shell.execute_reply.started":"2021-10-10T12:06:33.23676Z","shell.execute_reply":"2021-10-10T12:06:33.796384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function that removes stopwords\ndef stop_words(df, column_name):\n    df = df.copy()\n    # Returns tokenized words that are not rt\n    returns = lambda tweets: [i for i in tweets if i != 'rt']\n    df[column_name] = df[column_name].apply(returns)\n    \n    #Create a function stops which returns the words in a tokenized dataframe that do not appear in a stopwords set\n    stop_word = lambda tweets: [i for i in tweets if i not in sw('en')]\n    df[column_name] = df[column_name].apply(stop_word)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:33.798625Z","iopub.execute_input":"2021-10-10T12:06:33.79886Z","iopub.status.idle":"2021-10-10T12:06:33.804215Z","shell.execute_reply.started":"2021-10-10T12:06:33.798834Z","shell.execute_reply":"2021-10-10T12:06:33.803383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the stops function the tokenized testing dataset dataframe\ntrain_df_stopwords = stop_words(train_df_tokens, 'message')\n\nwith context:\n    display(train_df_stopwords.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:33.805549Z","iopub.execute_input":"2021-10-10T12:06:33.805765Z","iopub.status.idle":"2021-10-10T12:06:46.494032Z","shell.execute_reply.started":"2021-10-10T12:06:33.80574Z","shell.execute_reply":"2021-10-10T12:06:46.493007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_stopwords = stop_words(test_df_tokens, 'message')\n\nwith context:\n    display(test_df_stopwords.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:46.494926Z","iopub.execute_input":"2021-10-10T12:06:46.495158Z","iopub.status.idle":"2021-10-10T12:06:54.916201Z","shell.execute_reply.started":"2021-10-10T12:06:46.495122Z","shell.execute_reply":"2021-10-10T12:06:54.91554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to lemmatize words in training dataframe\ntrain_df_lemmatized = train_df_stopwords.copy()\n\ntrain_df_lemmatized['message'] = train_df_lemmatized['message'].apply(lambda sentence : [WordNetLemmatizer().lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maxmimum width for the message column\nwith context:\n    display(train_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:54.917431Z","iopub.execute_input":"2021-10-10T12:06:54.917654Z","iopub.status.idle":"2021-10-10T12:06:57.034452Z","shell.execute_reply.started":"2021-10-10T12:06:54.917629Z","shell.execute_reply":"2021-10-10T12:06:57.033529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to lemmatize words in training dataframe\ntest_df_lemmatized = test_df_stopwords.copy()\n\ntest_df_lemmatized['message'] = test_df_lemmatized['message'].apply(lambda sentence : [WordNetLemmatizer().lemmatize(word) for word in sentence])\n\n# Display the first 10 rows of the lemmatized_train dataframe, allowing maxmimum width for the message column\nwith context:\n    display(test_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.036035Z","iopub.execute_input":"2021-10-10T12:06:57.036399Z","iopub.status.idle":"2021-10-10T12:06:57.392991Z","shell.execute_reply.started":"2021-10-10T12:06:57.036358Z","shell.execute_reply":"2021-10-10T12:06:57.392394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge tokenized words into sentences (train_df_lammetized)\ntrain_df_lemmatized['message'] = [' '.join(i) for i in train_df_lemmatized['message'].values]\n\nwith context:\n    display(train_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.393874Z","iopub.execute_input":"2021-10-10T12:06:57.394551Z","iopub.status.idle":"2021-10-10T12:06:57.417114Z","shell.execute_reply.started":"2021-10-10T12:06:57.394514Z","shell.execute_reply":"2021-10-10T12:06:57.416206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge tokenized words into sentences (test_df_lammetized)\ntest_df_lemmatized['message'] = [' '.join(i) for i in test_df_lemmatized['message'].values]\n\nwith context:\n    display(test_df_lemmatized.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.418438Z","iopub.execute_input":"2021-10-10T12:06:57.418646Z","iopub.status.idle":"2021-10-10T12:06:57.436834Z","shell.execute_reply.started":"2021-10-10T12:06:57.418621Z","shell.execute_reply":"2021-10-10T12:06:57.435938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring Data (EDA)\n<a id='EDA'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"markdown","source":"* **Sentiment Dataframe**","metadata":{}},{"cell_type":"code","source":"# Grouping tweets by sentiment and display count in message column\nsentiment_df = train_df_lemmatized.groupby('sentiment').count()['message'].reset_index().sort_values(by = 'message', ascending = False)\nsentiment_df","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.438487Z","iopub.execute_input":"2021-10-10T12:06:57.438748Z","iopub.status.idle":"2021-10-10T12:06:57.469506Z","shell.execute_reply.started":"2021-10-10T12:06:57.438717Z","shell.execute_reply":"2021-10-10T12:06:57.468587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Bar Graph**","metadata":{}},{"cell_type":"code","source":"# Visualize number of tweets using a bar plot\nbar_graph = go.Figure(go.Bar(x = ['Positive', 'News', 'Neutral', 'Negative'],y = sentiment_df['message'], \n                       marker = {'color': sentiment_df['message'],'colorscale': 'Viridis'})) \nbar_graph.update_layout(yaxis_title = 'No. of Tweets', xaxis_title = 'Sentiment', title = 'No. of tweets per sentiment')\nbar_graph.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.470917Z","iopub.execute_input":"2021-10-10T12:06:57.471155Z","iopub.status.idle":"2021-10-10T12:06:57.637904Z","shell.execute_reply.started":"2021-10-10T12:06:57.47113Z","shell.execute_reply":"2021-10-10T12:06:57.636632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Cloud Words**","metadata":{}},{"cell_type":"code","source":"# Collecting words from different sentiments\npos_words = \" \".join([i for i in train_df_lemmatized['message'][train_df_lemmatized['sentiment'] == 1]])\nneg_words = \" \".join([y for y in train_df_lemmatized['message'][train_df_lemmatized['sentiment'] == -1]])\nneutral_words = \" \".join([i for i in train_df_lemmatized['message'][train_df_lemmatized['sentiment'] == 0]])\nnews_words = \" \".join([y for y in train_df_lemmatized['message'][train_df_lemmatized['sentiment'] == 2]])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.639321Z","iopub.execute_input":"2021-10-10T12:06:57.639708Z","iopub.status.idle":"2021-10-10T12:06:57.653827Z","shell.execute_reply.started":"2021-10-10T12:06:57.639657Z","shell.execute_reply":"2021-10-10T12:06:57.652678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of frequent words in wordscloud \nfreq_words = ['warming', 'change', 'climate', 'global']\nnew_pos = \" \".join([i for i in pos_words.split() if i not in freq_words])\nnew_neg = \" \".join([y for y in neg_words.split() if y not in freq_words])\nnew_neutral = \" \".join([i for i in neutral_words.split() if i not in freq_words])\nnew_news = \" \".join([y for y in news_words.split() if y not in freq_words])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.655526Z","iopub.execute_input":"2021-10-10T12:06:57.655913Z","iopub.status.idle":"2021-10-10T12:06:57.706007Z","shell.execute_reply.started":"2021-10-10T12:06:57.655871Z","shell.execute_reply":"2021-10-10T12:06:57.705037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Hashtags Extraction**","metadata":{}},{"cell_type":"code","source":"# Hash_tags function to extract hashtags\ndef hashtag_function(tweet):\n    hash_tags = []\n    for i in tweet: \n        tags = re.findall(r\"#(\\w+)\", i)\n        hash_tags.append(tags)\n    return hash_tags","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.707638Z","iopub.execute_input":"2021-10-10T12:06:57.708072Z","iopub.status.idle":"2021-10-10T12:06:57.713038Z","shell.execute_reply.started":"2021-10-10T12:06:57.708037Z","shell.execute_reply":"2021-10-10T12:06:57.712191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting hashtags associated to positive, negative, neutral and news class\npos_tags = hashtag_function(train['message'][train['sentiment'] == 1])\nneg_tags = hashtag_function(train['message'][train['sentiment'] == -1])\nneutral_tags = hashtag_function(train['message'][train['sentiment'] == 0])\nnews_tags = hashtag_function(train['message'][train['sentiment'] == 2])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.714331Z","iopub.execute_input":"2021-10-10T12:06:57.714595Z","iopub.status.idle":"2021-10-10T12:06:57.751645Z","shell.execute_reply.started":"2021-10-10T12:06:57.714566Z","shell.execute_reply":"2021-10-10T12:06:57.750584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list for every sentiment\npos_tags = sum(pos_tags, [])\nneg_tags = sum(neg_tags, [])\nneutral_tags = sum(neutral_tags, [])\nnews_tags = sum(news_tags, [])\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.753175Z","iopub.execute_input":"2021-10-10T12:06:57.753432Z","iopub.status.idle":"2021-10-10T12:06:57.802871Z","shell.execute_reply.started":"2021-10-10T12:06:57.753403Z","shell.execute_reply":"2021-10-10T12:06:57.80175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Displaying the most frequent words in positive, negative, neutral and news hashtags list\nword_pos = nltk.FreqDist(pos_tags)\nword_neg = nltk.FreqDist(neg_tags)\nword_neutral = nltk.FreqDist(neutral_tags)\nword_news = nltk.FreqDist(news_tags)\n\n#Dataframes\nword_pos_df = pd.DataFrame({'Hashtags' : list(word_pos.keys()),'Count' : list(word_pos.values())})\nword_neg_df = pd.DataFrame({'Hashtags' : list(word_neg.keys()),'Count' : list(word_neg.values())})\nword_neutral_df = pd.DataFrame({'Hashtags' : list(word_neutral.keys()),'Count' : list(word_neutral.values())})\nword_news_df = pd.DataFrame({'Hashtags' : list(word_news.keys()),'Count' : list(word_news.values())})\n\n# Sorting in descending order\nword_pos_df_sorted = word_pos_df.sort_values(by = \"Count\", ascending = False)\nword_neg_df_sorted = word_neg_df.sort_values(by = \"Count\", ascending = False)\nword_neutral_df_sorted = word_neutral_df.sort_values(by = \"Count\", ascending = False)\nword_news_df_sorted = word_news_df.sort_values(by = \"Count\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.804253Z","iopub.execute_input":"2021-10-10T12:06:57.804513Z","iopub.status.idle":"2021-10-10T12:06:57.825561Z","shell.execute_reply.started":"2021-10-10T12:06:57.804484Z","shell.execute_reply":"2021-10-10T12:06:57.824562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of most frequent words of positive sentiment\nword_pos_df_sorted.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.827129Z","iopub.execute_input":"2021-10-10T12:06:57.827374Z","iopub.status.idle":"2021-10-10T12:06:57.841746Z","shell.execute_reply.started":"2021-10-10T12:06:57.827347Z","shell.execute_reply":"2021-10-10T12:06:57.840616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of most frequent words of negative sentiment\nword_neg_df_sorted.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.846249Z","iopub.execute_input":"2021-10-10T12:06:57.846534Z","iopub.status.idle":"2021-10-10T12:06:57.860326Z","shell.execute_reply.started":"2021-10-10T12:06:57.846505Z","shell.execute_reply":"2021-10-10T12:06:57.859332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of most frequent words of news sentiment\nword_news_df_sorted.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.861763Z","iopub.execute_input":"2021-10-10T12:06:57.862089Z","iopub.status.idle":"2021-10-10T12:06:57.876316Z","shell.execute_reply.started":"2021-10-10T12:06:57.862049Z","shell.execute_reply":"2021-10-10T12:06:57.875236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display first 10 rows of most frequent words of news sentiment\nword_neutral_df_sorted.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.877647Z","iopub.execute_input":"2021-10-10T12:06:57.878034Z","iopub.status.idle":"2021-10-10T12:06:57.89186Z","shell.execute_reply.started":"2021-10-10T12:06:57.877991Z","shell.execute_reply":"2021-10-10T12:06:57.890948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Words extraction**","metadata":{}},{"cell_type":"code","source":"# Lambda function for extracting words\nwords_extractor = lambda words:  \" \".join([i for i in words.split() if i not in freq_words])\n\nword_pos = train_df_lemmatized[train_df_lemmatized['sentiment'] == 1] \nword_pos = word_pos['message'].apply(words_extractor)\nword_neg = train_df_lemmatized[train_df_lemmatized['sentiment'] == -1]\nword_neg = word_neg['message'].apply(words_extractor)\nword_neutral = train_df_lemmatized[train_df_lemmatized['sentiment'] == 0]\nword_neutral = word_neutral['message'].apply(words_extractor)\nword_news = train_df_lemmatized[train_df_lemmatized['sentiment'] == 2]\nword_news = word_news['message'].apply(words_extractor)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.893339Z","iopub.execute_input":"2021-10-10T12:06:57.893747Z","iopub.status.idle":"2021-10-10T12:06:57.941453Z","shell.execute_reply.started":"2021-10-10T12:06:57.893715Z","shell.execute_reply":"2021-10-10T12:06:57.940425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Call CountVectorizer**","metadata":{}},{"cell_type":"code","source":"# Use CountVectorizer to transform word_positive, word_negative, word_neutral and word_news\n# word_positive\ncountV_pos = CountVectorizer()\ndocs_positive = countV_pos.fit_transform(word_pos)\nfeatures_positive = countV_pos.get_feature_names()\n\n# word_negative\ncountV_neg = CountVectorizer()\ndocs_negative = countV_neg.fit_transform(word_neg)\nfeatures_negative = countV_neg.get_feature_names()\n\n# word_neutral\ncountV_neutral = CountVectorizer()\ndocs_neutral = countV_neutral.fit_transform(word_neutral)\nfeatures_neutral = countV_neutral.get_feature_names()\n\n# word_news\ncountV_news = CountVectorizer()\ndocs_news = countV_news.fit_transform(word_news)\nfeatures_news = countV_news.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:57.94256Z","iopub.execute_input":"2021-10-10T12:06:57.94279Z","iopub.status.idle":"2021-10-10T12:06:58.116678Z","shell.execute_reply.started":"2021-10-10T12:06:57.942756Z","shell.execute_reply":"2021-10-10T12:06:58.115871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Frequency Distribution Visualizer**","metadata":{}},{"cell_type":"code","source":"# Display frequency distribution of top 10 tokens for positive sentiment'\nfDist_pos = FreqDistVisualizer(features = features_positive, orient = 'v', n = 10, \n            color = 'm', title = 'frequency_distribution of top 10 tokens for positive sentiment')\nvisual_pos = RadViz(classes = docs_positive, features = features_positive, size = (800, 420))\n\nfDist_pos.fit(docs_positive)\nfDist_pos.show() ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:58.118071Z","iopub.execute_input":"2021-10-10T12:06:58.118357Z","iopub.status.idle":"2021-10-10T12:06:58.400299Z","shell.execute_reply.started":"2021-10-10T12:06:58.118327Z","shell.execute_reply":"2021-10-10T12:06:58.399421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display frequency distribution of top 10 tokens for negative sentiment'\nfDist_neg = FreqDistVisualizer(features = features_negative, orient = 'v', n = 10, \n            color = 'c', title = 'frequency_distribution of top 10 tokens for negative sentiment')\nvisual_neg = RadViz(classes = docs_negative, features = features_negative, size = (800, 420))\n\nfDist_neg.fit(docs_negative)\nfDist_neg.show() ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:58.401481Z","iopub.execute_input":"2021-10-10T12:06:58.401694Z","iopub.status.idle":"2021-10-10T12:06:58.913392Z","shell.execute_reply.started":"2021-10-10T12:06:58.40167Z","shell.execute_reply":"2021-10-10T12:06:58.912408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display frequency distribution of top 10 tokens for neutral sentiment'\nfDist_neutral = FreqDistVisualizer(features = features_neutral, orient = 'v', n = 10, \n            color = 'b', title = 'frequency_distribution of top 10 tokens for negative sentiment')\nvisual_neutral = RadViz(classes = docs_neutral, features = features_neutral, size = (800, 420))\n\nfDist_neutral.fit(docs_neutral)\nfDist_neutral.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:58.914702Z","iopub.execute_input":"2021-10-10T12:06:58.915567Z","iopub.status.idle":"2021-10-10T12:06:59.159751Z","shell.execute_reply.started":"2021-10-10T12:06:58.915537Z","shell.execute_reply":"2021-10-10T12:06:59.159072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display frequency distribution of top 10 tokens for news sentiment\nfDist_news = FreqDistVisualizer(features = features_news, orient = 'v', n = 10, \n            color = 'r', title = 'frequency_distribution of top 10 tokens for news sentiment')\nvisual_news = RadViz(classes = docs_news, features = features_news, size = (800, 420))\n\nfDist_news.fit(docs_news)\nfDist_news.show() ","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:59.16092Z","iopub.execute_input":"2021-10-10T12:06:59.161652Z","iopub.status.idle":"2021-10-10T12:06:59.393681Z","shell.execute_reply.started":"2021-10-10T12:06:59.16162Z","shell.execute_reply":"2021-10-10T12:06:59.392808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature  Engineering\n<a id='extraction'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"code","source":"def preprocessing(message):\n    \n    # set to lower cases\n    str_msg = message.lower()\n    str_msg = re.sub(r\"http\\S+\", \"\", str_msg)\n    \n    # tokenize string\n    tokenized_str = TweetTokenizer(strip_handles = True)\n    str_msg = tokenized_str.tokenize(str_msg)\n    \n    # join and extract string.\n    str_msg = \" \".join(str_msg)\n    str_msg = re.sub(r'[^a-z0-9\\s]', '', str_msg)\n    str_msg = re.sub(r'[0-9]+', '', str_msg)\n    \n    tweet = re.sub(r'^rt', '', str_msg)\n    return tweet","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:06:59.394985Z","iopub.execute_input":"2021-10-10T12:06:59.395315Z","iopub.status.idle":"2021-10-10T12:06:59.402377Z","shell.execute_reply.started":"2021-10-10T12:06:59.395277Z","shell.execute_reply":"2021-10-10T12:06:59.40149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count Vectorizing the train dataset\ntrain_df = train.copy()\n#train_df['message']= train_df['message'].apply(preprocessing)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:12.980211Z","iopub.execute_input":"2021-10-10T12:41:12.980887Z","iopub.status.idle":"2021-10-10T12:41:12.987141Z","shell.execute_reply.started":"2021-10-10T12:41:12.980844Z","shell.execute_reply":"2021-10-10T12:41:12.986174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:14.177394Z","iopub.execute_input":"2021-10-10T12:41:14.177676Z","iopub.status.idle":"2021-10-10T12:41:14.18921Z","shell.execute_reply.started":"2021-10-10T12:41:14.177645Z","shell.execute_reply":"2021-10-10T12:41:14.188355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare variable x and y\nx = train_df['message']\ny = train_df['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:29.086942Z","iopub.execute_input":"2021-10-10T12:41:29.088931Z","iopub.status.idle":"2021-10-10T12:41:29.093535Z","shell.execute_reply.started":"2021-10-10T12:41:29.088888Z","shell.execute_reply":"2021-10-10T12:41:29.092843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call CountVectorizer \ncount_vector = CountVectorizer(ngram_range =(1,2))\nX = count_vector.fit_transform(x)\n\nX.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:48.454669Z","iopub.execute_input":"2021-10-10T12:41:48.455111Z","iopub.status.idle":"2021-10-10T12:41:49.500701Z","shell.execute_reply.started":"2021-10-10T12:41:48.455062Z","shell.execute_reply":"2021-10-10T12:41:49.499928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Train Test Split**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:52.495928Z","iopub.execute_input":"2021-10-10T12:41:52.496807Z","iopub.status.idle":"2021-10-10T12:41:52.50692Z","shell.execute_reply.started":"2021-10-10T12:41:52.496748Z","shell.execute_reply":"2021-10-10T12:41:52.506011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Modelling\n<a id='modelling'></a>\n   [Back to table of contents](#table)","metadata":{}},{"cell_type":"markdown","source":"*  **Logistic Regression Classifier**","metadata":{}},{"cell_type":"code","source":"# Declare logistic Regression Classifier \nLog_reg_class = LogisticRegression(multi_class = 'auto', max_iter=100, solver = 'liblinear', \n                                   random_state = 42).fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:26.928723Z","iopub.execute_input":"2021-10-10T12:44:26.929005Z","iopub.status.idle":"2021-10-10T12:44:30.37266Z","shell.execute_reply.started":"2021-10-10T12:44:26.928979Z","shell.execute_reply":"2021-10-10T12:44:30.371674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the predictions\npred_log_reg = Log_reg_class.predict(X_test)\npred_log_reg","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:31.991276Z","iopub.execute_input":"2021-10-10T12:44:31.992158Z","iopub.status.idle":"2021-10-10T12:44:32.001032Z","shell.execute_reply.started":"2021-10-10T12:44:31.992108Z","shell.execute_reply":"2021-10-10T12:44:32.000416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking model performance with F1 score\nacc_model = f1_score(y_test,pred_log_reg,average =\"weighted\") \nacc_model","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:35.24507Z","iopub.execute_input":"2021-10-10T12:44:35.246017Z","iopub.status.idle":"2021-10-10T12:44:35.255253Z","shell.execute_reply.started":"2021-10-10T12:44:35.245969Z","shell.execute_reply":"2021-10-10T12:44:35.254485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n* **Support Vector Classifier**","metadata":{}},{"cell_type":"code","source":"# Declare support vector classifier model\nSupp_Vect_Class = SVC(C = 12, gamma = 0.01).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:48.417018Z","iopub.execute_input":"2021-10-10T12:44:48.417949Z","iopub.status.idle":"2021-10-10T12:46:05.203778Z","shell.execute_reply.started":"2021-10-10T12:44:48.41791Z","shell.execute_reply":"2021-10-10T12:46:05.203136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the predictions\npred_supp_vect = Supp_Vect_Class.predict(X_test)\npred_supp_vect","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:46:05.205035Z","iopub.execute_input":"2021-10-10T12:46:05.205752Z","iopub.status.idle":"2021-10-10T12:46:14.129764Z","shell.execute_reply.started":"2021-10-10T12:46:05.205718Z","shell.execute_reply":"2021-10-10T12:46:14.128824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking model performance with F1 score\nacc_model2 = f1_score(y_test,pred_supp_vect,average =\"weighted\") \nacc_model2","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:46:14.130855Z","iopub.execute_input":"2021-10-10T12:46:14.131104Z","iopub.status.idle":"2021-10-10T12:46:14.140957Z","shell.execute_reply.started":"2021-10-10T12:46:14.131065Z","shell.execute_reply":"2021-10-10T12:46:14.140229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **SMOTE Naive Bayes Classifier**","metadata":{}},{"cell_type":"code","source":"# Declare SMOTE Naive Bayes Classifier\nNaive_Bayes_Class = MultinomialNB().fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:01.987951Z","iopub.execute_input":"2021-10-10T12:44:01.988259Z","iopub.status.idle":"2021-10-10T12:44:02.016122Z","shell.execute_reply.started":"2021-10-10T12:44:01.988229Z","shell.execute_reply":"2021-10-10T12:44:02.01531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the predictions\npred_naive_bayes = Naive_Bayes_Class.predict(X_test)\npred_naive_bayes","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:02.777829Z","iopub.execute_input":"2021-10-10T12:44:02.778768Z","iopub.status.idle":"2021-10-10T12:44:02.791031Z","shell.execute_reply.started":"2021-10-10T12:44:02.778725Z","shell.execute_reply":"2021-10-10T12:44:02.790111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking model performance with F1 score\nacc_model3 = f1_score(y_test,pred_log_reg,average =\"weighted\") \nacc_model3","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:44:03.787605Z","iopub.execute_input":"2021-10-10T12:44:03.787872Z","iopub.status.idle":"2021-10-10T12:44:03.797645Z","shell.execute_reply.started":"2021-10-10T12:44:03.787846Z","shell.execute_reply":"2021-10-10T12:44:03.79676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"Support Vector Classifier model is performing better than others.  ","metadata":{}},{"cell_type":"markdown","source":"# 7. Model Results","metadata":{}},{"cell_type":"code","source":"# Model test data \ntest_df = test.copy()\n#test_df['message'] = test_df['message'].apply(preprocessing)\ntest_count_vector =  count_vector.transform(test_df['message'])","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:47:07.790146Z","iopub.execute_input":"2021-10-10T12:47:07.790816Z","iopub.status.idle":"2021-10-10T12:47:08.168749Z","shell.execute_reply.started":"2021-10-10T12:47:07.790779Z","shell.execute_reply":"2021-10-10T12:47:08.168141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict on test_data\npred_supp_vect_sub = Supp_Vect_Class.predict(test_count_vector)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:47:12.540997Z","iopub.execute_input":"2021-10-10T12:47:12.541819Z","iopub.status.idle":"2021-10-10T12:47:40.167008Z","shell.execute_reply.started":"2021-10-10T12:47:12.541783Z","shell.execute_reply":"2021-10-10T12:47:40.166225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the submission dataframe\nsubmission = pd.DataFrame({'tweetid' : test_df['tweetid'], \n                           'sentiment' : pred_supp_vect_sub})\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:49:29.896982Z","iopub.execute_input":"2021-10-10T12:49:29.897326Z","iopub.status.idle":"2021-10-10T12:49:29.908543Z","shell.execute_reply.started":"2021-10-10T12:49:29.897292Z","shell.execute_reply":"2021-10-10T12:49:29.907914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save submision file on csv.\nsubmission.to_csv(\"Bote_Mkwanazi_Classification_Prediction.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:49:34.706998Z","iopub.execute_input":"2021-10-10T12:49:34.70784Z","iopub.status.idle":"2021-10-10T12:49:34.730455Z","shell.execute_reply.started":"2021-10-10T12:49:34.707803Z","shell.execute_reply":"2021-10-10T12:49:34.7297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}